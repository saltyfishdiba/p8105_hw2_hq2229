p8105_hw2_hq2229
================
Hantang Qin
2025-09-28

Problem1

Clean csv

``` r
## P1 — libraries
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.1
    ## ✔ ggplot2   3.5.2     ✔ tibble    3.3.0
    ## ✔ lubridate 1.9.4     ✔ tidyr     1.3.1
    ## ✔ purrr     1.1.0     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(lubridate)
library(janitor)
```

    ## 
    ## 载入程序包：'janitor'
    ## 
    ## The following objects are masked from 'package:stats':
    ## 
    ##     chisq.test, fisher.test

``` r
## P1 — paths (use relative for grading)
p1_dir  <- "D:/BISTP8105/p8105_hw2_hq2229/fivethirtyeight_datasets"
pols_fp <- file.path(p1_dir, "pols-month.csv")
snp_fp  <- file.path(p1_dir, "snp.csv")
un_fp   <- file.path(p1_dir, "unemployment.csv")

## helper: month number -> jan..dec (lowercase)
to_mon_abb <- function(m) tolower(month.abb[as.integer(m)])

## 1a. pols: split date, set president, drop day & prez*_flags
pols <- read_csv(pols_fp, show_col_types = FALSE) |>
  clean_names() |>
  separate(mon, into = c("year","month","day"), sep = "-", convert = TRUE) |>
  mutate(
    month     = to_mon_abb(month),
    president = case_when(prez_gop == 1 ~ "gop",
                          prez_dem == 1 ~ "dem",
                          TRUE          ~ NA_character_)
  ) |>
  select(year, month, gov_gop, gov_dem, sen_gop, sen_dem, rep_gop, rep_dem, president) |>
  arrange(year, match(month, tolower(month.abb)))

## 1b. snp: parse date, produce year/month cols, keep close
snp <- read_csv(snp_fp, show_col_types = FALSE) |>
  clean_names() |>
  mutate(date = mdy(date),
         year = year(date),
         month = tolower(month.abb[month(date)])) |>
  select(year, month, close) |>
  arrange(year, match(month, tolower(month.abb)))

## 1c. unemployment: wide->long, standardize names/values
unemp <- read_csv(un_fp, show_col_types = FALSE) |>
  pivot_longer(-Year, names_to = "month", values_to = "unemployment_rate") |>
  clean_names() |>
  mutate(month = tolower(month)) |>
  rename(year = year) |>
  arrange(year, match(month, tolower(month.abb)))

## 1d. join all by (year, month)
p1_final <- pols |>
  left_join(snp,   by = c("year","month")) |>
  left_join(unemp, by = c("year","month"))

## 1e. quick descriptors for write-up
p1_n_obs  <- nrow(p1_final)
p1_n_vars <- ncol(p1_final)
p1_year_min <- min(p1_final$year, na.rm = TRUE)
p1_year_max <- max(p1_final$year, na.rm = TRUE)
```

Working through the first dataset felt a little like piecing together a
giant puzzle. The pols-month file was all about politics: it gave me
month-by-month counts of governors, senators, and representatives, and
it even let me mark which party the sitting president belonged to. Then
I added in the S&P dataset, which was basically the stock market’s
monthly “mood ring,” showing the closing value of the S&P index.
Finally, I brought in the unemployment data to see how the U.S. job
market was shifting over time.

After cleaning and merging everything by year and month, I ended up with
a tidy panel that has ***r p1_n_obs*** observations and ***r
p1_n_vars*** variables, covering the years ***r p1_year_min***–***r
p1_year_max***. What I really liked here is how the final dataset pulls
together three different parts of American life—politics, the economy,
and employment—into one table. It feels kind of powerful: with just a
few lines of R code, I now have a dataset where I could ask questions
like, “How did the stock market move under different presidents?” or
“What was happening with unemployment when Congress looked a certain
way?” This is exactly the kind of connection between data and real life
that makes me excited about biostatistics.

Problem 2

``` r
library(tidyverse)
library(readxl)
library(janitor)
library(lubridate)

# Relative path (per HW)
trash_xlsx <- "202409 Trash Wheel Collection Data.xlsx"

# Helper: read a sheet using an explicit range to omit non-data rows/cols
read_sheet_rng <- function(sheet, range) {
  if (sheet %in% excel_sheets(trash_xlsx)) {
    read_excel(trash_xlsx, sheet = sheet, range = range, .name_repair = "minimal") |>
      select(where(~ !all(is.na(.))))  # drop any all-NA columns just in case
  } else {
    NULL
  }
}

# Standard cleaner applied to each wheel
tw_clean <- function(df, label) {
  if (is.null(df)) return(NULL)
  out <- df |>
    clean_names() |>
    mutate(
      # parse date; infer year/month if not provided
      date  = as.Date(date),
      year  = suppressWarnings(as.integer(year)),
      month = suppressWarnings(as.integer(month)),
      year  = if_else(is.na(year)  & !is.na(date), lubridate::year(date),  year),
      month = if_else(is.na(month) & !is.na(date), lubridate::month(date), month)
    )
  # round sports balls if present
  if ("sports_balls" %in% names(out)) {
    out <- out |> mutate(sports_balls = as.integer(round(sports_balls)))
  }
  out |>
    select(any_of(c(
      "dumpster","date","year","month","weight_tons","volume_cubic_yards",
      "plastic_bottles","polystyrene","cigarette_butts","glass_bottles",
      "grocery_bags","chip_bags","sports_balls","homes_powered"
    ))) |>
    mutate(wheel = label) |>
    relocate(wheel)
}

# Read + clean each wheel (ranges trimmed to data area to skip notes/headers)
mr   <- read_sheet_rng("Mr. Trash Wheel",        "A2:N653") |> tw_clean("Mr. Trash Wheel")
prof <- read_sheet_rng("Professor Trash Wheel",  "A2:M120") |> tw_clean("Professor Trash Wheel")
gwyn <- read_sheet_rng("Gwynnda Trash Wheel",    "A2:L265") |> tw_clean("Gwynnda Trash Wheel")

# Bind all wheels — tidy, comparable columns
trash_all <- list(mr, prof, gwyn) |>
  purrr::compact() |>
  list_rbind() |>
  arrange(wheel, year, month, dumpster)

# === Required answers ===
tw_n_obs <- nrow(trash_all)
tw_n_devices <- n_distinct(trash_all$wheel)

tw_prof_total_tons <- trash_all |>
  filter(wheel == "Professor Trash Wheel") |>
  summarise(total = sum(weight_tons, na.rm = TRUE)) |>
  pull(total) |>
  round(1)

tw_gwyn_jun22_cigs <- trash_all |>
  filter(wheel == "Gwynnda Trash Wheel", year == 2022, month == 6) |>
  summarise(total = sum(cigarette_butts, na.rm = TRUE)) |>
  pull(total)
```

This part felt like cleaning three mini kitchens so we could cook one
big meal. Each Trash Wheel sheet had slightly different quirks, so I
standardized the names, trimmed off the non-data rows by reading
explicit ranges, and (when needed) pulled `year` and `month` straight
from the `date`. I also rounded `sports_balls` to the nearest integer so
it behaves like a count.

After stacking everything together, the combined dataset has **769**
dumpster-level observations across **2** wheels (Mr., Professor, and
Gwynnda). Core variables include `date`, `weight_tons`,
`volume_cubic_yards`, and item counts like `plastic_bottles`,
`cigarette_butts`, and `glass_bottles`. For the required summaries:
**Professor Trash Wheel** has collected a total of **246.7** tons, and
**Gwynnda** picked up **0** cigarette butts in **June 2022**. It’s neat
seeing how much cleaner the harbor story becomes once the data is
tidy—the numbers start to feel real.

Problem 3

``` r
library(tidyverse)
library(janitor)
library(lubridate)
library(knitr)
library(stringr)
library(dplyr)

# --- Helper: NYC county -> borough ---
borough_map <- c(
  "New York" = "Manhattan",
  "Kings"    = "Brooklyn",
  "Queens"   = "Queens",
  "Bronx"    = "Bronx",
  "Richmond" = "Staten Island"
)

# --- Read & clean ZIP reference (neighborhoods, counties) ---
zip <- read_csv("zillow_data/Zip Codes.csv", na = c("NA", ".", ""),
                show_col_types = FALSE) |>
  clean_names() |>
  mutate(
    zip_code = as.character(zip_code),
    county   = as.character(county)
  )

# --- Read & clean ZORI (wide) ---
price_wide <- read_csv("zillow_data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv",
                       na = c("NA", ".", ""), show_col_types = FALSE) |>
  clean_names() |>
  mutate(county_name = str_remove(county_name, " County$")) |>
  rename(zip_code = region_name, county = county_name) |>
  # remove leading "x" from date columns
  rename_with(~ str_remove(.x, "^x"), .cols = starts_with("x")) |>
  # drop rows entirely NA across all date columns
  filter(!if_all(matches("^\\d{4}_\\d{2}_\\d{2}$"), is.na)) |>
  # keep identifiers + all date columns
  select(zip_code, county, region_id, size_rank, matches("^\\d{4}_\\d{2}_\\d{2}$")) |>
  mutate(
    zip_code = as.character(zip_code),
    county   = as.character(county)
  )

# --- Build final tidy dataset (long), with parsed Date + borough ---
joint_data <- zip |>
  # include all ZIPs from the reference; prices may be NA
  left_join(price_wide, by = c("zip_code","county")) |>
  pivot_longer(
    cols      = matches("^\\d{4}_\\d{2}_\\d{2}$"),
    names_to  = "date_chr",
    values_to = "price"
  ) |>
  mutate(
    date    = ymd(str_replace_all(date_chr, "_", "-")),
    borough = recode(county, !!!borough_map)
  ) |>
  select(any_of(c(
    "state","city","county","borough","state_fips","county_fips","county_code","metro",
    "neighborhood","zip_code","region_id","size_rank","date","price","file_date"
  ))) |>
  arrange(zip_code, date)

# --- Required descriptors for write-up ---
p3_n_obs          <- nrow(joint_data)
p3_n_zip          <- n_distinct(joint_data$zip_code)
p3_n_neighborhood <- n_distinct(joint_data$neighborhood)
p3_date_min       <- suppressWarnings(min(joint_data$date, na.rm = TRUE))
p3_date_max       <- suppressWarnings(max(joint_data$date, na.rm = TRUE))

# --- ZIPs present in ZIP file but missing from ZORI (match on BOTH keys) ---
zips_missing <- dplyr::anti_join(
  zip |> dplyr::select(any_of(c("zip_code","county","neighborhood","city","state"))),
  price_wide |> dplyr::select(zip_code, county),
  by = c("zip_code","county")
)
zips_missing_examples <- zips_missing |> slice_head(n = 10)

# --- Largest drop from Jan 2020 to Jan 2021 (drop = 2020 - 2021) ---
drops_20_21 <- price_wide |>
  transmute(
    zip_code, county,
    drop_2020_to_2021 = `2020_01_31` - `2021_01_31`
  ) |>
  filter(!is.na(drop_2020_to_2021)) |>
  left_join(zip |> select(zip_code, any_of("neighborhood")), by = "zip_code") |>
  mutate(borough = recode(county, !!!borough_map))

top10_largest_drop <- drops_20_21 |>
  arrange(desc(drop_2020_to_2021)) |>
  select(zip_code, borough, neighborhood, county, drop_2020_to_2021) |>
  slice_head(n = 10)

# --- (Optional) Biggest overall range across 2015–2024 (silence NaN groups) ---
top10_range <- joint_data |>
  group_by(zip_code) |>
  filter(any(!is.na(price))) |>
  summarise(
    price_max   = max(price, na.rm = TRUE),
    price_min   = min(price, na.rm = TRUE),
    price_range = price_max - price_min,
    .groups = "drop"
  ) |>
  arrange(desc(price_range)) |>
  slice_head(n = 10) |>
  left_join(zip |> select(zip_code, any_of(c("neighborhood","county"))), by = "zip_code") |>
  mutate(borough = recode(county, !!!borough_map)) |>
  relocate(borough, neighborhood, county, .after = zip_code)

# --- Nicely formatted tables for the Rmd ---
kable(top10_largest_drop,
     caption = "Top 10 ZIP codes by largest ZORI drop (Jan 2020 → Jan 2021; drop = 2020 − 2021)")
```

| zip_code | borough   | neighborhood                  | county   | drop_2020_to_2021 |
|:---------|:----------|:------------------------------|:---------|------------------:|
| 10007    | Manhattan | Lower Manhattan               | New York |          912.5966 |
| 10069    | Manhattan | NA                            | New York |          748.1245 |
| 10009    | Manhattan | Lower East Side               | New York |          714.2550 |
| 10016    | Manhattan | Gramercy Park and Murray Hill | New York |          711.7045 |
| 10001    | Manhattan | Chelsea and Clinton           | New York |          710.4499 |
| 10002    | Manhattan | Lower East Side               | New York |          710.3028 |
| 10004    | Manhattan | Lower Manhattan               | New York |          705.9608 |
| 10038    | Manhattan | Lower Manhattan               | New York |          697.5853 |
| 10012    | Manhattan | Greenwich Village and Soho    | New York |          686.2218 |
| 10010    | Manhattan | Gramercy Park and Murray Hill | New York |          684.9304 |

Top 10 ZIP codes by largest ZORI drop (Jan 2020 → Jan 2021; drop = 2020
− 2021)

``` r
kable(zips_missing_examples,
     caption = "Examples: ZIPs present in ZIP file but missing from ZORI dataset")
```

| zip_code | county | neighborhood               |
|:---------|:-------|:---------------------------|
| 10464    | Bronx  | Southeast Bronx            |
| 10474    | Bronx  | Hunts Point and Mott Haven |
| 10475    | Bronx  | Northeast Bronx            |
| 10499    | Bronx  | NA                         |
| 10550    | Bronx  | NA                         |
| 10704    | Bronx  | NA                         |
| 10705    | Bronx  | NA                         |
| 10803    | Bronx  | NA                         |
| 11202    | Kings  | NA                         |
| 11224    | Kings  | Southern Brooklyn          |

Examples: ZIPs present in ZIP file but missing from ZORI dataset

``` r
kable(top10_range,
     caption = "Top 10 ZIP codes by overall ZORI range (Jan 2015 – Aug 2024)")
```

| zip_code | borough   | neighborhood               | county   | price_max | price_min | price_range |
|:---------|:----------|:---------------------------|:---------|----------:|----------:|------------:|
| 10007    | Manhattan | Lower Manhattan            | New York |  8422.624 |  5421.614 |    3001.010 |
| 10069    | Manhattan | NA                         | New York |  5926.113 |  3874.918 |    2051.196 |
| 10013    | Manhattan | Greenwich Village and Soho | New York |  5739.831 |  3794.151 |    1945.680 |
| 10014    | Manhattan | Greenwich Village and Soho | New York |  5032.078 |  3188.487 |    1843.591 |
| 10038    | Manhattan | Lower Manhattan            | New York |  4696.795 |  2875.616 |    1821.179 |
| 10011    | Manhattan | Chelsea and Clinton        | New York |  5228.192 |  3428.366 |    1799.826 |
| 10001    | Manhattan | Chelsea and Clinton        | New York |  5176.020 |  3377.574 |    1798.446 |
| 10004    | Manhattan | Lower Manhattan            | New York |  4223.940 |  2443.697 |    1780.243 |
| 10012    | Manhattan | Greenwich Village and Soho | New York |  4700.332 |  2942.344 |    1757.988 |
| 10282    | Manhattan | NA                         | New York |  7074.194 |  5349.640 |    1724.555 |

Top 10 ZIP codes by overall ZORI range (Jan 2015 – Aug 2024)

This step felt like turning two messy spreadsheets into one NYC story. I
cleaned the ZIP reference file and the Zillow rent data, matched them by
ZIP + county, and converted all those wide date columns into a single
timeline. After parsing a real date and adding a borough label (e.g.,
Kings → Brooklyn), I ended up with a tidy dataset of **r p3_n_obs** rows
spanning **r p3_date_min** to **r p3_date_max**, covering **r p3_n_zip**
ZIP codes and **r p3_n_neighborhood** neighborhoods. A few ZIPs appear
in the ZIP file but not in Zillow—likely PO boxes, non-residential
areas, or gaps in coverage (see the example table). The biggest Jan-2020
→ Jan-2021 rent drops line up with what we’d expect from the pandemic
years, especially in core boroughs. It’s cool how a little tidying turns
raw files into something you can actually reason about.
