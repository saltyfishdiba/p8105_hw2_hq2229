---
title: "p8105_hw2_hq2229"
author: "Hantang Qin"
output: github_document
date: "2025-09-28"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Problem1 

Clean csv
```{r}

library(tidyverse)
library(lubridate)
library(janitor)

p1_dir  <- "D:/BISTP8105/p8105_hw2_hq2229/fivethirtyeight_datasets"
pols_fp <- file.path(p1_dir, "pols-month.csv")
snp_fp  <- file.path(p1_dir, "snp.csv")
un_fp   <- file.path(p1_dir, "unemployment.csv")

to_mon_abb <- function(m) tolower(month.abb[as.integer(m)])

##split date
pols <- read_csv(pols_fp, show_col_types = FALSE) |>
  clean_names() |>
  separate(mon, into = c("year","month","day"), sep = "-", convert = TRUE) |>
  mutate(
    month     = to_mon_abb(month),
    president = case_when(prez_gop == 1 ~ "gop",
                          prez_dem == 1 ~ "dem",
                          TRUE          ~ NA_character_)
  ) |>
  select(year, month, gov_gop, gov_dem, sen_gop, sen_dem, rep_gop, rep_dem, president) |>
  arrange(year, match(month, tolower(month.abb)))

## snp: parse date, produce year/month cols
snp <- read_csv(snp_fp, show_col_types = FALSE) |>
  clean_names() |>
  mutate(date = mdy(date),
         year = year(date),
         month = tolower(month.abb[month(date)])) |>
  select(year, month, close) |>
  arrange(year, match(month, tolower(month.abb)))

##  unemployment: wide->long
unemp <- read_csv(un_fp, show_col_types = FALSE) |>
  pivot_longer(-Year, names_to = "month", values_to = "unemployment_rate") |>
  clean_names() |>
  mutate(month = tolower(month)) |>
  rename(year = year) |>
  arrange(year, match(month, tolower(month.abb)))

##(year, month)
p1_final <- pols |>
  left_join(snp,   by = c("year","month")) |>
  left_join(unemp, by = c("year","month"))

##descriptors write-up
p1_n_obs  <- nrow(p1_final)
p1_n_vars <- ncol(p1_final)
p1_year_min <- min(p1_final$year, na.rm = TRUE)
p1_year_max <- max(p1_final$year, na.rm = TRUE)


```
Working through the first dataset felt a little like piecing together a giant puzzle. The pols-month file was all about politics: it gave me month-by-month counts of governors, senators, and representatives, and it even let me mark which party the sitting president belonged to. Then I added in the S&P dataset, which was basically the stock market’s monthly “mood ring,” showing the closing value of the S&P index. Finally, I brought in the unemployment data to see how the U.S. job market was shifting over time.

After cleaning and merging everything by year and month, I ended up with a tidy panel that has ***r p1_n_obs*** observations and ***r p1_n_vars*** variables, covering the years ***r p1_year_min***–***r p1_year_max***. What I really liked here is how the final dataset pulls together three different parts of American life—politics, the economy, and employment—into one table. It feels kind of powerful: with just a few lines of R code, I now have a dataset where I could ask questions like, “How did the stock market move under different presidents?” or “What was happening with unemployment when Congress looked a certain way?” This is exactly the kind of connection between data and real life that makes me excited about biostatistics.


Problem 2
```{r}
library(tidyverse)
library(readxl)
library(janitor)
library(lubridate)

trash_xlsx <- "202409 Trash Wheel Collection Data.xlsx"

read_sheet_rng <- function(sheet, range) {
  if (sheet %in% excel_sheets(trash_xlsx)) {
    read_excel(trash_xlsx, sheet = sheet, range = range, .name_repair = "minimal") |>
      select(where(~ !all(is.na(.))))  # drop any all-NA columns just in case
  } else {
    NULL
  }
}

# Standard cleaner applied to each wheel
tw_clean <- function(df, label) {
  if (is.null(df)) return(NULL)
  out <- df |>
    clean_names() |>
    mutate(
      # parse date; infer year/month if not provided
      date  = as.Date(date),
      year  = suppressWarnings(as.integer(year)),
      month = suppressWarnings(as.integer(month)),
      year  = if_else(is.na(year)  & !is.na(date), lubridate::year(date),  year),
      month = if_else(is.na(month) & !is.na(date), lubridate::month(date), month)
    )
  # round sports balls if present
  if ("sports_balls" %in% names(out)) {
    out <- out |> mutate(sports_balls = as.integer(round(sports_balls)))
  }
  out |>
    select(any_of(c(
      "dumpster","date","year","month","weight_tons","volume_cubic_yards",
      "plastic_bottles","polystyrene","cigarette_butts","glass_bottles",
      "grocery_bags","chip_bags","sports_balls","homes_powered"
    ))) |>
    mutate(wheel = label) |>
    relocate(wheel)
}

# Read + clean 
mr   <- read_sheet_rng("Mr. Trash Wheel",        "A2:N653") |> tw_clean("Mr. Trash Wheel")
prof <- read_sheet_rng("Professor Trash Wheel",  "A2:M120") |> tw_clean("Professor Trash Wheel")
gwyn <- read_sheet_rng("Gwynnda Trash Wheel",    "A2:L265") |> tw_clean("Gwynnda Trash Wheel")

# Bind all wheels tidy comparable columns
trash_all <- list(mr, prof, gwyn) |>
  purrr::compact() |>
  list_rbind() |>
  arrange(wheel, year, month, dumpster)
###
tw_n_obs <- nrow(trash_all)
tw_n_devices <- n_distinct(trash_all$wheel)

tw_prof_total_tons <- trash_all |>
  filter(wheel == "Professor Trash Wheel") |>
  summarise(total = sum(weight_tons, na.rm = TRUE)) |>
  pull(total) |>
  round(1)

tw_gwyn_jun22_cigs <- trash_all |>
  filter(wheel == "Gwynnda Trash Wheel", year == 2022, month == 6) |>
  summarise(total = sum(cigarette_butts, na.rm = TRUE)) |>
  pull(total)




```



This part felt like cleaning three mini kitchens so we could cook one big meal. Each Trash Wheel sheet had slightly different quirks, so I standardized the names, trimmed off the non-data rows by reading explicit ranges, and (when needed) pulled `year` and `month` straight from the `date`. I also rounded `sports_balls` to the nearest integer so it behaves like a count.  

After stacking everything together, the combined dataset has **`r tw_n_obs`** dumpster-level observations across **`r tw_n_devices`** wheels (Mr., Professor, and Gwynnda). Core variables include `date`, `weight_tons`, `volume_cubic_yards`, and item counts like `plastic_bottles`, `cigarette_butts`, and `glass_bottles`. For the required summaries: **Professor Trash Wheel** has collected a total of **`r tw_prof_total_tons`** tons, and **Gwynnda** picked up **`r tw_gwyn_jun22_cigs`** cigarette butts in **June 2022**. It’s neat seeing how much cleaner the harbor story becomes once the data is tidy—the numbers start to feel real.




Problem 3
```{r}
library(tidyverse)
library(janitor)
library(lubridate)
library(knitr)
library(stringr)
library(dplyr)

#  NYC county -> borough 
borough_map <- c(
  "New York" = "Manhattan",
  "Kings"    = "Brooklyn",
  "Queens"   = "Queens",
  "Bronx"    = "Bronx",
  "Richmond" = "Staten Island"
)

# Read & clean ZIP reference (neighborhoods, counties)
zip <- read_csv("zillow_data/Zip Codes.csv", na = c("NA", ".", ""),
                show_col_types = FALSE) |>
  clean_names() |>
  mutate(
    zip_code = as.character(zip_code),
    county   = as.character(county)
  )

# Read & clean ZORI 
price_wide <- read_csv("zillow_data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv",
                       na = c("NA", ".", ""), show_col_types = FALSE) |>
  clean_names() |>
  mutate(county_name = str_remove(county_name, " County$")) |>
  rename(zip_code = region_name, county = county_name) |>
  # remove leading "x" from date columns
  rename_with(~ str_remove(.x, "^x"), .cols = starts_with("x")) |>
  # drop rows entirely NA across all date columns
  filter(!if_all(matches("^\\d{4}_\\d{2}_\\d{2}$"), is.na)) |>
  # keep identifiers + all date columns
  select(zip_code, county, region_id, size_rank, matches("^\\d{4}_\\d{2}_\\d{2}$")) |>
  mutate(
    zip_code = as.character(zip_code),
    county   = as.character(county)
  )

#  tidy dataset (long), with parsed Date + borough 
joint_data <- zip |>
  # include all ZIPs from the reference; prices may be NA
  left_join(price_wide, by = c("zip_code","county")) |>
  pivot_longer(
    cols      = matches("^\\d{4}_\\d{2}_\\d{2}$"),
    names_to  = "date_chr",
    values_to = "price"
  ) |>
  mutate(
    date    = ymd(str_replace_all(date_chr, "_", "-")),
    borough = recode(county, !!!borough_map)
  ) |>
  select(any_of(c(
    "state","city","county","borough","state_fips","county_fips","county_code","metro",
    "neighborhood","zip_code","region_id","size_rank","date","price","file_date"
  ))) |>
  arrange(zip_code, date)

#descriptors 
p3_n_obs          <- nrow(joint_data)
p3_n_zip          <- n_distinct(joint_data$zip_code)
p3_n_neighborhood <- n_distinct(joint_data$neighborhood)
p3_date_min       <- suppressWarnings(min(joint_data$date, na.rm = TRUE))
p3_date_max       <- suppressWarnings(max(joint_data$date, na.rm = TRUE))

# ZIPs present 
zips_missing <- dplyr::anti_join(
  zip |> dplyr::select(any_of(c("zip_code","county","neighborhood","city","state"))),
  price_wide |> dplyr::select(zip_code, county),
  by = c("zip_code","county")
)
zips_missing_examples <- zips_missing |> slice_head(n = 10)

# --- Largest drop from Jan 2020 to Jan 2021 (drop = 2020 - 2021) ---
drops_20_21 <- price_wide |>
  transmute(
    zip_code, county,
    drop_2020_to_2021 = `2020_01_31` - `2021_01_31`
  ) |>
  filter(!is.na(drop_2020_to_2021)) |>
  left_join(zip |> select(zip_code, any_of("neighborhood")), by = "zip_code") |>
  mutate(borough = recode(county, !!!borough_map))

top10_largest_drop <- drops_20_21 |>
  arrange(desc(drop_2020_to_2021)) |>
  select(zip_code, borough, neighborhood, county, drop_2020_to_2021) |>
  slice_head(n = 10)

# Biggest overall range across 2015–2024 (silence NaN groups)
top10_range <- joint_data |>
  group_by(zip_code) |>
  filter(any(!is.na(price))) |>
  summarise(
    price_max   = max(price, na.rm = TRUE),
    price_min   = min(price, na.rm = TRUE),
    price_range = price_max - price_min,
    .groups = "drop"
  ) |>
  arrange(desc(price_range)) |>
  slice_head(n = 10) |>
  left_join(zip |> select(zip_code, any_of(c("neighborhood","county"))), by = "zip_code") |>
  mutate(borough = recode(county, !!!borough_map)) |>
  relocate(borough, neighborhood, county, .after = zip_code)

#formatted tables for Rmd
kable(top10_largest_drop,
     caption = "Top 10 ZIP codes by largest ZORI drop (Jan 2020 → Jan 2021; drop = 2020 − 2021)")

kable(zips_missing_examples,
     caption = "Examples: ZIPs present in ZIP file but missing from ZORI dataset")

kable(top10_range,
     caption = "Top 10 ZIP codes by overall ZORI range (Jan 2015 – Aug 2024)")

```

This step felt like turning two messy spreadsheets into one NYC story. I cleaned the ZIP reference file and the Zillow rent data, matched them by ZIP + county, and converted all those wide date columns into a single timeline. After parsing a real date and adding a borough label (e.g., Kings → Brooklyn), I ended up with a tidy dataset of **r p3_n_obs** rows spanning **r p3_date_min** to **r p3_date_max**, covering **r p3_n_zip** ZIP codes and **r p3_n_neighborhood** neighborhoods. A few ZIPs appear in the ZIP file but not in Zillow—likely PO boxes, non-residential areas, or gaps in coverage (see the example table). The biggest Jan-2020 → Jan-2021 rent drops line up with what we’d expect from the pandemic years, especially in core boroughs. It’s cool how a little tidying turns raw files into something you can actually reason about.
